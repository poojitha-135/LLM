{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUAYmhVqF3v1",
        "outputId": "7e028c3a-71ba-4212-aa4d-43bc49cf4e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = [\n",
        "    \"This is word tokenization.\",\n",
        "    \"NLTK is used for NLP.\"\n",
        "]\n",
        "tokenized_text = [word_tokenize(s) for s in text]\n",
        "print(\"Tokenized text:\")\n",
        "for i, tokens in enumerate(tokenized_text):\n",
        "    print(f\"s {i+1}: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZSb8PrxHlSU",
        "outputId": "e969f18c-f546-492c-acbc-3e05be9989e1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized text:\n",
            "s 1: ['This', 'is', 'word', 'tokenization', '.']\n",
            "s 2: ['NLTK', 'is', 'used', 'for', 'NLP', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW5HxG1OIX14",
        "outputId": "81c56785-3ba0-45f9-e647-754ed8d4fbf7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Embeddings\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "text = [\n",
        "    \"This is word tokenization.\",\n",
        "    \"NLTK is used for NLP.\"\n",
        "]\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in text]\n",
        "print(\"Tokenized text:\")\n",
        "for i, tokens in enumerate(tokenized_text):\n",
        "    print(f\"Sentence {i+1}: {tokens}\")\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_text,\n",
        "    vector_size=50,\n",
        "    window=2,\n",
        "    min_count=1,\n",
        "    workers=1,\n",
        "    sg=0\n",
        ")\n",
        "print(\"\\nWord Embeddings:\\n\")\n",
        "for word in model.wv.index_to_key:\n",
        "    print(f\"{word}: {model.wv[word]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO0Wp4JrH02B",
        "outputId": "de18ff90-1bc4-4c07-9854-a4e28fa99e71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized text:\n",
            "Sentence 1: ['This', 'is', 'word', 'tokenization', '.']\n",
            "Sentence 2: ['NLTK', 'is', 'used', 'for', 'NLP', '.']\n",
            "\n",
            "Word Embeddings:\n",
            "\n",
            ".: [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n",
            "is: [-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
            "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
            " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
            " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
            "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
            " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
            "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
            "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
            "  0.00180291  0.01278507]\n",
            "NLP: [-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
            "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
            " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
            " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
            "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
            "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
            "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
            " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
            "  0.00302193  0.00358008]\n",
            "for: [ 1.56351421e-02 -1.90203730e-02 -4.11062239e-04  6.93839323e-03\n",
            " -1.87794445e-03  1.67635437e-02  1.80215668e-02  1.30730132e-02\n",
            " -1.42324204e-03  1.54208085e-02 -1.70686692e-02  6.41421322e-03\n",
            " -9.27599426e-03 -1.01779103e-02  7.17923651e-03  1.07406788e-02\n",
            "  1.55390287e-02 -1.15330126e-02  1.48667218e-02  1.32509926e-02\n",
            " -7.41960062e-03 -1.74912829e-02  1.08749345e-02  1.30195115e-02\n",
            " -1.57510047e-03 -1.34197120e-02 -1.41718509e-02 -4.99412045e-03\n",
            "  1.02865072e-02 -7.33047491e-03 -1.87401194e-02  7.65347946e-03\n",
            "  9.76895820e-03 -1.28571270e-02  2.41711619e-03 -4.14975407e-03\n",
            "  4.88066689e-05 -1.97670180e-02  5.38400887e-03 -9.50021297e-03\n",
            "  2.17529293e-03 -3.15244915e-03  4.39334614e-03 -1.57631524e-02\n",
            " -5.43436781e-03  5.32639725e-03  1.06933638e-02 -4.78302967e-03\n",
            " -1.90201886e-02  9.01175756e-03]\n",
            "used: [ 0.00018913  0.00615464 -0.01362529 -0.00275093  0.01533716  0.01469282\n",
            " -0.00734659  0.0052854  -0.01663426  0.01241097 -0.00927464 -0.00632821\n",
            "  0.01862271  0.00174677  0.01498141 -0.01214813  0.01032101  0.01984565\n",
            " -0.01691478 -0.01027138 -0.01412967 -0.0097253  -0.00755713 -0.0170724\n",
            "  0.01591121 -0.00968788  0.01684723  0.01052514 -0.01310005  0.00791574\n",
            "  0.0109403  -0.01485307 -0.01481144 -0.00495046 -0.01725145 -0.00316314\n",
            " -0.00080687  0.00659937  0.00288376 -0.00176284 -0.01118812  0.00346073\n",
            " -0.00179474  0.01358738  0.00794718  0.00905894  0.00286861 -0.00539971\n",
            " -0.00873363 -0.00206415]\n",
            "NLTK: [ 2.8740549e-03 -5.2920175e-03 -1.4147566e-02 -1.5610614e-02\n",
            " -1.8243574e-02 -1.1870339e-02 -3.6948491e-03 -8.6477427e-03\n",
            " -1.2921341e-02 -7.4346447e-03  8.5783172e-03 -7.4780867e-03\n",
            "  1.6756350e-02  3.0679870e-03 -1.4484639e-02  1.8867597e-02\n",
            "  1.5262425e-02  1.0986564e-02 -1.3697691e-02  1.1645358e-02\n",
            "  8.0181863e-03  1.0370739e-02  8.5118031e-03  3.8795089e-03\n",
            " -6.3403249e-03  1.6707690e-02  1.9224361e-02  7.5852061e-03\n",
            " -5.6739901e-03  1.4255047e-05  2.4376370e-03 -1.6916649e-02\n",
            " -1.6447891e-02 -4.6203137e-04  2.4745751e-03 -1.1486761e-02\n",
            " -9.4505474e-03 -1.4692149e-02  1.6657231e-02  2.4259568e-04\n",
            " -9.0187974e-03  1.1403411e-02  1.8360030e-02 -8.1997439e-03\n",
            "  1.5929364e-02  1.0750868e-02  1.1758246e-02  1.0251808e-03\n",
            "  1.6426168e-02 -1.4038081e-02]\n",
            "tokenization: [-0.01648536  0.01859871 -0.00039532 -0.00393455  0.00920726 -0.00819063\n",
            "  0.00548623  0.01387993  0.01213085 -0.01502159  0.0187647   0.00934362\n",
            "  0.00793224 -0.01248701  0.01691996 -0.00430033  0.01765038 -0.01072401\n",
            " -0.01625884  0.01364912  0.00334239 -0.00439702  0.0190272   0.01898771\n",
            " -0.01954809  0.00501046  0.01231338  0.00774491  0.00404557  0.000861\n",
            "  0.00134726 -0.00764127 -0.0142805  -0.00417774  0.0078478   0.01763737\n",
            "  0.0185183  -0.01195187 -0.01880534  0.01952875  0.00685957  0.01033223\n",
            "  0.01256469 -0.00560853  0.01464541  0.00566054  0.00574201 -0.00476074\n",
            " -0.0062565  -0.00474028]\n",
            "word: [ 0.00855287  0.00015212 -0.01916856 -0.01933109 -0.01229639 -0.00025714\n",
            "  0.00399483  0.01886394  0.0111687  -0.00858139  0.00055663  0.00992872\n",
            "  0.01539662 -0.00228845  0.00864684 -0.01162876 -0.00160838  0.0162001\n",
            " -0.00472013 -0.01932691  0.01155852 -0.00785964 -0.00244575  0.01996103\n",
            " -0.0045127  -0.00951413 -0.01065877  0.01396178 -0.01141774  0.00422733\n",
            " -0.01051132  0.01224143  0.00871461  0.00521271 -0.00298217 -0.00549213\n",
            "  0.01798587  0.01043155 -0.00432504 -0.01894062 -0.0148521  -0.00212748\n",
            " -0.00158989 -0.00512582  0.01936544 -0.00091704  0.01174752 -0.01489517\n",
            " -0.00501215 -0.01109973]\n",
            "This: [-0.01427803  0.00248206 -0.01435343 -0.00448924  0.00743861  0.01166625\n",
            "  0.00239637  0.00420546 -0.00822078  0.01445067 -0.01261408  0.00929443\n",
            " -0.01643995  0.00407294 -0.0099541  -0.00849538 -0.00621797  0.01131042\n",
            "  0.0115968  -0.0099493   0.00154666 -0.01699156  0.01561961  0.01851458\n",
            " -0.00548466  0.00160045  0.0014933   0.01095577 -0.01721216  0.00116891\n",
            "  0.01373884  0.00446319  0.00224935 -0.01864431  0.01696473 -0.01252825\n",
            " -0.00598475  0.00698757 -0.00154526  0.00282258  0.00356398 -0.0136578\n",
            " -0.01944962  0.01808117  0.01239611 -0.01382586  0.00680696  0.00041213\n",
            "  0.00950749 -0.01423989]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch scikit-learn numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XasTg7HjRkSe",
        "outputId": "5ea946e2-a826-4915-9bba-8d9f5778d24e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Text similarity detections\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModel.from_pretrained(model_id)\n",
        "def avg_pooling(output, mask):\n",
        "    embeddings = output.last_hidden_state\n",
        "    mask = mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "    return (embeddings * mask).sum(1) / torch.clamp(mask.sum(1), min=1e-9)\n",
        "def sentence_vector(text):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "    return avg_pooling(output, inputs[\"attention_mask\"]).numpy()\n",
        "def similarity_score(s1, s2):\n",
        "    v1 = sentence_vector(s1)\n",
        "    v2 = sentence_vector(s2)\n",
        "    return cosine_similarity(v1, v2)[0][0]\n",
        "sentence1 = input(\"Enter sentence 1: \")\n",
        "sentence2 = input(\"Enter sentence 2: \")\n",
        "score = similarity_score(sentence1, sentence2)\n",
        "print(score)\n",
        "if score >= 0.5:\n",
        "    print(\"Similar\")\n",
        "else:\n",
        "    print(\"Not Similar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA9SCmaKQ1w_",
        "outputId": "3d5146a7-382d-4aa5-b42e-1da13a29e9ee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the first sentence: Deep learning is a subset of machine learning\n",
            "Enter the second sentence: Machine learning includes deep learning techniques\n",
            "Similarity Score: 0.77538013\n",
            "The sentences are similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Sentimental Analysis\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "ml_dl_texts = [\n",
        "    \"Machine learning models perform exceptionally well on this dataset\",\n",
        "    \"The deep learning training process was slow and inefficient\",\n",
        "    \"I am impressed by the accuracy of this neural network\",\n",
        "    \"The model failed to converge and produced poor results\",\n",
        "    \"This deep learning framework makes development much easier\"\n",
        "]\n",
        "output = sentiment_analyzer(ml_dl_texts)\n",
        "for sentence, res in zip(ml_dl_texts, output):\n",
        "    print(\"Text:\", sentence)\n",
        "    print(\"Sentiment:\", res[\"label\"], \"Confidence:\", round(res[\"score\"], 2))\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWg8yDrhRVV2",
        "outputId": "2a739405-9534-4dfc-8ea3-c300430d374b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Machine learning models perform exceptionally well on this dataset\n",
            "Sentiment: POSITIVE Confidence: 1.0\n",
            "--------------------------------------------------\n",
            "Text: The deep learning training process was slow and inefficient\n",
            "Sentiment: NEGATIVE Confidence: 1.0\n",
            "--------------------------------------------------\n",
            "Text: I am impressed by the accuracy of this neural network\n",
            "Sentiment: POSITIVE Confidence: 1.0\n",
            "--------------------------------------------------\n",
            "Text: The model failed to converge and produced poor results\n",
            "Sentiment: NEGATIVE Confidence: 1.0\n",
            "--------------------------------------------------\n",
            "Text: This deep learning framework makes development much easier\n",
            "Sentiment: POSITIVE Confidence: 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Fake news classifier\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "fake_news_detector = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"jy46604790/Fake-News-Bert-Detect\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "ml_dl_news = [\n",
        "    \"Researchers developed a new deep learning model that improves image recognition accuracy.\",\n",
        "    \"A machine learning algorithm has become self-aware and plans to control the world.\",\n",
        "    \"Scientists use AI and deep learning to detect cancer at an early stage.\",\n",
        "    \"Training a neural network for five minutes can make anyone a data science expert.\"\n",
        "]\n",
        "predictions = fake_news_detector(ml_dl_news)\n",
        "\n",
        "for article, result in zip(ml_dl_news, predictions):\n",
        "    print(\"News:\", article)\n",
        "    for item in result:\n",
        "        print(\"Label:\", item[\"label\"], \"Confidence:\", round(item[\"score\"], 2))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTrJtoqCY9RI",
        "outputId": "24f2c997-f675-4738-e081-87e19b01d21d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News: Researchers developed a new deep learning model that improves image recognition accuracy.\n",
            "Label: LABEL_0 Confidence: 1.0\n",
            "Label: LABEL_1 Confidence: 0.0\n",
            "------------------------------------------------------------\n",
            "News: A machine learning algorithm has become self-aware and plans to control the world.\n",
            "Label: LABEL_0 Confidence: 1.0\n",
            "Label: LABEL_1 Confidence: 0.0\n",
            "------------------------------------------------------------\n",
            "News: Scientists use AI and deep learning to detect cancer at an early stage.\n",
            "Label: LABEL_0 Confidence: 0.99\n",
            "Label: LABEL_1 Confidence: 0.01\n",
            "------------------------------------------------------------\n",
            "News: Training a neural network for five minutes can make anyone a data science expert.\n",
            "Label: LABEL_0 Confidence: 1.0\n",
            "Label: LABEL_1 Confidence: 0.0\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmIMgUBXY_x6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}