{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhTsCigGidCO/5VldcVCgw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojitha-135/LLM/blob/main/input_pipeline(llm).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQDMzofy1ZPI",
        "outputId": "a9217c96-8998-4fbf-a5bb-0f41bae90212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Tokens\n",
            "['Architectures', 'in', 'a', 'llm']\n",
            "\n",
            "Step 2: Token IDs\n",
            "Architectures → 0\n",
            "in → 1\n",
            "a → 2\n",
            "llm → 3\n",
            "\n",
            "Step 3: Token Embeddings\n",
            "\n",
            "Architectures token embedding:\n",
            "tensor([ 1.5753,  0.9956, -0.4382, -0.8278,  0.1510, -0.0030, -1.3857, -0.0956],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "in token embedding:\n",
            "tensor([ 1.1817,  0.5014, -1.0012, -0.5865,  0.0785, -1.3120,  0.4565, -0.6219],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "a token embedding:\n",
            "tensor([ 0.2597,  0.1420, -2.1856, -0.2566,  2.9763,  2.7537, -0.1780,  0.2411],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "llm token embedding:\n",
            "tensor([ 0.5663,  1.2422,  0.7175,  0.2247, -0.2903, -0.4005, -1.3815, -1.5078],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Step 4: Positional Embeddings\n",
            "\n",
            "Position 0 embedding:\n",
            "tensor([-1.2337,  2.2709, -0.2260, -0.8339,  0.0372, -0.3234, -0.2746, -1.0020],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Position 1 embedding:\n",
            "tensor([ 0.6856, -0.1911, -0.4081,  0.1439,  0.3352,  1.3068,  1.4433, -1.1544],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Position 2 embedding:\n",
            "tensor([-0.9657,  0.2255, -1.4578,  1.3994, -0.0193, -0.3927, -0.8857, -1.0177],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Position 3 embedding:\n",
            "tensor([ 1.3081, -0.4878, -0.1890,  0.7564,  0.1340,  0.1571,  1.3532, -0.2539],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Step 5: Final Input Vectors (Token + Position)\n",
            "\n",
            "Final input vector for 'Architectures':\n",
            "tensor([ 0.3416,  3.2665, -0.6642, -1.6617,  0.1882, -0.3265, -1.6603, -1.0976],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Final input vector for 'in':\n",
            "tensor([ 1.8673,  0.3102, -1.4093, -0.4426,  0.4138, -0.0052,  1.8998, -1.7764],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Final input vector for 'a':\n",
            "tensor([-0.7060,  0.3675, -3.6434,  1.1428,  2.9569,  2.3609, -1.0637, -0.7766],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "\n",
            "Final input vector for 'llm':\n",
            "tensor([ 1.8744,  0.7543,  0.5285,  0.9810, -0.1562, -0.2434, -0.0282, -1.7617],\n",
            "       grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# Step 1: Tokenization\n",
        "sentence = \"Architectures in a llm\"\n",
        "tokens = sentence.split()\n",
        "print(\"Step 1: Tokens\")\n",
        "print(tokens)\n",
        "# Step 2: Token IDs\n",
        "vocab = {token: idx for idx, token in enumerate(tokens)}\n",
        "token_ids = torch.tensor([vocab[token] for token in tokens])\n",
        "print(\"\\nStep 2: Token IDs\")\n",
        "for token, idx in vocab.items():\n",
        "    print(f\"{token} → {idx}\")\n",
        "# Step 3: Token Embeddings\n",
        "embedding_dim = 8\n",
        "token_embedding_layer = nn.Embedding(len(vocab), embedding_dim)\n",
        "token_embeddings = token_embedding_layer(token_ids)\n",
        "print(\"\\nStep 3: Token Embeddings\")\n",
        "for token, emb in zip(tokens, token_embeddings):\n",
        "    print(f\"\\n{token} token embedding:\\n{emb}\")\n",
        "# Step 4: Positional Embeddings\n",
        "max_len = len(tokens)\n",
        "position_ids = torch.arange(max_len)\n",
        "positional_embedding_layer = nn.Embedding(max_len, embedding_dim)\n",
        "positional_embeddings = positional_embedding_layer(position_ids)\n",
        "print(\"\\nStep 4: Positional Embeddings\")\n",
        "for i, pos_emb in enumerate(positional_embeddings):\n",
        "    print(f\"\\nPosition {i} embedding:\\n{pos_emb}\")\n",
        "# Step 5: Final Input Vectors\n",
        "# (Token Embedding + Positional Embedding)\n",
        "final_input_vectors = token_embeddings + positional_embeddings\n",
        "print(\"\\nStep 5: Final Input Vectors (Token + Position)\")\n",
        "for token, final_vec in zip(tokens, final_input_vectors):\n",
        "    print(f\"\\nFinal input vector for '{token}':\\n{final_vec}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "id": "-c-9W9ay1aII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}